# stable/kube-prometheus-stack

grafana:
  enabled: true

  persistence:
    enabled: true

  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
    datasources:
      enabled: true
      defaultDatasourceEnabled: true
      label: grafana_datasource

  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
    hosts:
      - grafana-2c98ao.dev.live-smart.io
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana-2c98ao.dev.live-smart.io

  adminPassword: Sr1m0JDA^14esHF

additionalPrometheusRules:
  - name: ls-app
    groups:
      - name: Unhandled errors
        rules:
          - expr: increase(ls_app_exceptions[30s]) > 3
            alert: LsMiddlewareUnhandledExceptionsHigh
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: High number of errors in application
              description: "Errors in `{{$labels.instance}}` of job `{{$labels.job}}`."
          - expr: increase(ls_app_rejections[30s]) > 3
            alert: LsMiddlewareUnhandledRejectionsHigh
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: High number of errors in application
              description: "Errors in `{{$labels.instance}}` of job `{{$labels.job}}`."
          - expr: rabbitmq_queue_messages_ready > 50
            alert: LsMiddlewareRMQMessagesHigh
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: High number of messages ready for processing in RabbitMQ
              description: "Messages in `{{$labels.queue}}`."
          - expr: http_request_duration_seconds_count{job=~"ls.*web"} < 1
            alert: LsMiddlewareRequestsLow
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: No HTTP requests detected for web service.
              description: "Service affected `{{$labels.job}}`."
          - expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[30s])) by (le)) > 2
            alert: LsMiddlewareResponseTimeHigh
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: Average response time for HTTP is > 2s.
              description: "Service affected `{{$labels.job}}`."

alertmanager:
  ## Deploy alertmanager
  ##
  enabled: true

  config:
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
        - match:
            alertname: Watchdog
          receiver: 'null'
        - receiver: 'kube-adm'
    receivers:
      - name: 'null'
      - name: 'kube-adm'
        email_configs:
          - send_resolved: true
            to: "test@email.com"
            from: "alertmanager@email.com"
            smarthost: mailhog.b2b-network:1025
            require_tls: false

  ## Service account for Alertmanager to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    create: true

  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  ## Pass the Alertmanager configuration directives through Helm's templating
  ## engine. If the Alertmanager configuration contains Alertmanager templates,
  ## they'll need to be properly escaped so that they are not interpreted by
  ## Helm
  ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function
  ##      https://prometheus.io/docs/alerting/configuration/#%3Ctmpl_string%3E
  ##      https://prometheus.io/docs/alerting/notifications/
  ##      https://prometheus.io/docs/alerting/notification_examples/
  tplConfig: false

  ## Alertmanager template files to format alerts
  ## ref: https://prometheus.io/docs/alerting/notifications/
  ##      https://prometheus.io/docs/alerting/notification_examples/
  ##
  templateFiles: {}
  #
  ## An example template:
  #   template_1.tmpl: |-
  #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
  #
  #       {{ define "slack.myorg.text" }}
  #       {{- $root := . -}}
  #       {{ range .Alerts }}
  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
  #         *Cluster:*  {{ template "cluster" $root }}
  #         *Description:* {{ .Annotations.description }}
  #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
  #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
  #         *Details:*
  #           {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
  #           {{ end }}

  ingress:
    enabled: true

    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/auth-secret: network/nginx-auth

    hosts:
      - am-2c98ao.dev.live-smart.io

    paths:
      - /

    tls:
      - secretName: alertmanager-web-tls
        hosts:
          - am-2c98ao.dev.live-smart.io

  serviceMonitor:
    selfMonitor: true

    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # 	relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   target_label: nodename
    #   replacement: $1
    #   action: replace

  alertmanagerSpec:
    image:
      repository: quay.io/prometheus/alertmanager
      tag: v0.21.0

    logFormat: logfmt

    replicas: 1

    retention: 120h

    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi

    routePrefix: /

    resources:
      requests:
        memory: 400Mi
        cpu: 100m
      limits:
        memory: 500Mi
        cpu: 300m

    podAntiAffinity: ""
    podAntiAffinityTopologyKey: kubernetes.io/hostname


prometheus:
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/auth-secret: network/nginx-auth

    hosts:
      - prometheus-2c98ao.dev.live-smart.io

    paths:
      - /

    tls:
      - secretName: prometheus-web-tls
        hosts:
          - prometheus-2c98ao.dev.live-smart.io

  server:
    persistentVolume:
      enabled: true
      size: 30Gi

  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false

    resources:
      requests:
        memory: 900Mi
        cpu: "1"
      limits:
        memory: 2000Mi
        cpu: "2"

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: standard
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi

prometheus-node-exporter:
  resources:
    limits:
      cpu: 200m
      memory: 100Mi
    requests:
      cpu: 100m
      memory: 40Mi

prometheusOperator:
  resources:
    limits:
      cpu: 800m
      memory: 400Mi
    requests:
      cpu: 500m
      memory: 200Mi


kubelet:
  #   enabled: true
  #   namespace: kube-system

  serviceMonitor:
    https: true

coreDns:
  enabled: false

defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    general: true
    k8s: true
    kubeApiserver: true
    kubePrometheusNodeAlerting: true
    kubePrometheusNodeRecording: true
    kubeScheduler: false
    kubernetesAbsent: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    node: true
    prometheusOperator: true
    prometheus: true

kubeScheduler:
  enabled: false

kubeControllerManager:
  enabled: false